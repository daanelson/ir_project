{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Model training for DRMM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.regularizers import l1, activity_l1, l2\n",
    "from keras.layers import Input, LSTM, Dense, merge, Dropout\n",
    "from keras.models import Model\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import make_test_data\n",
    "import pickle as pkl\n",
    "import os\n",
    "import pdb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#TODO: fix root, remove key restrictions\n",
    "\n",
    "# Placeholder data parsing\n",
    "def get_data(histograms, training_year=2014, training=True):\n",
    "    # Proposed format: [doc_id topic_id h0 h1 h2 h3 ...]\n",
    "    #data_root = '/scratch/cluster/dnelson/ir_proj'\n",
    "\n",
    "    # can set this to 2015 to read in annotations for 2015 instead\n",
    "    # format: label_dict[doc_id][topic_id] = ground truth\n",
    "    label_dict = make_test_data.make_truth(training_year)\n",
    "\n",
    "    # enforcing order on a dictionary & downsampling to only data w/judgments for training\n",
    "    key_array = histograms.keys()\n",
    "    if training:\n",
    "        key_array = [val for val in key_array if label_dict[int(val[0])][int(val[1])+1] >= 0]\n",
    "\n",
    "    X = np.array([[hist_list[1] for hist_list in histograms[key]] for key in key_array])\n",
    "    Y = np.array([label_dict[int(key[0])][int(key[1])+1] for key in key_array])\n",
    "    \n",
    "    MAX_LEN = 25\n",
    "    X = [val + [np.zeros(29)]*(MAX_LEN-len(val)) if len(val) < 25 else val[:25] for val in X]\n",
    "    return X, Y, key_array\n",
    "\n",
    "def get_dict(training_year=2014):\n",
    "    data_root = '/Users/Dan/class/deep_ir/project/data'\n",
    "    \n",
    "    with open(os.path.join(data_root, 'term_histograms_%d' % training_year), 'r') as f:\n",
    "        histograms = pkl.load(f)\n",
    "    return histograms\n",
    "\n",
    "\n",
    "def get_fake_data():\n",
    "    X_train = np.random.rand(1000, 29)\n",
    "\n",
    "    # roughly uniform dist of 0, 0.5, 1\n",
    "    rand_vals = np.random.rand(1000)\n",
    "    Y_train = np.array([np.floor(val * 3)/2.0 for val in rand_vals])\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all data for maximum iteration funtime power\n",
    "# THIS IS THE LONG PART\n",
    "train_dict = get_dict(training_year=2014)\n",
    "test_dict = get_dict(training_year=2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_training_data(X_train, Y_train):\n",
    "    new_data = []\n",
    "    MAX_ZERO = 1650\n",
    "    count = 0\n",
    "    for current_sample in zip(X_train, Y_train):\n",
    "        if current_sample[1] > 0:\n",
    "            new_data.append(current_sample)\n",
    "        elif count < MAX_ZERO:\n",
    "            new_data.append(current_sample)\n",
    "            count +=1\n",
    "    \n",
    "    np.random.shuffle(new_data)\n",
    "    X_new_train, Y_new_train = zip(*new_data)\n",
    "    return X_new_train, Y_new_train        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qrels2014.txt\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, _ = get_data(train_dict, training_year=2014, training=True)\n",
    "# COMMENT OUT THE BELOW LINE IF YOU DO NOT WANT TO RUN WITH FILTERED DATA\n",
    "X_train, Y_train = filter_training_data(X_train, Y_train)\n",
    "# X_train needs to be a list of numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_train_list = [X_train[:,val,:] for val in range(X_train.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qrels-treceval-2015.txt\n"
     ]
    }
   ],
   "source": [
    "#X_train, Y_train = get_fake_data()\n",
    "X_test, Y_test, test_keys = get_data(test_dict, training_year=2015, training=False)\n",
    "X_test = np.array(X_test)\n",
    "X_test_list = [X_test[:,val,:] for val in range(X_test.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5006\n"
     ]
    }
   ],
   "source": [
    "print len(X_train_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ASIDE - this is how you make your own loss function. Not necessary but fun to play with.\n",
    "from keras import backend as K\n",
    "\n",
    "def supercool_loss(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true*10 - y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, activity_l1, l2\n",
    "from keras.layers import Input, LSTM, Dense, merge, Dropout, Reshape, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create model (input_shape is inferred after first layer)\n",
    "# This model is a regression\n",
    "\n",
    "# Defines all shared weight layers for feedforward network\n",
    "num_inputs = 25\n",
    "shared_dense = Dense(5, activation='relu')\n",
    "shared_dropout = Dropout(0.5)\n",
    "shared_score = Dense(1, activation='relu')\n",
    "\n",
    "# constructs parallel shared layers for each input\n",
    "inputs = [Input(shape=(29,)) for val in range(num_inputs)]\n",
    "output_1 = [shared_dense(val) for val in inputs]\n",
    "output_d = [shared_dropout(val) for val in output_1]\n",
    "output_score = [shared_score(val) for val in output_d]\n",
    "concat_vals = merge(output_score, mode='concat')\n",
    "\n",
    "# Defines term weighting network\n",
    "shared_term_weight = Dense(1, activation='relu')\n",
    "term_weight_list = [shared_term_weight(val) for val in inputs]\n",
    "term_weights = merge(term_weight_list, mode='concat')\n",
    "\n",
    "# Dot product over terms to weight them properly\n",
    "output = merge([concat_vals, term_weights], mode='mul')\n",
    "sum_layer = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(1,))\n",
    "final_output = sum_layer(output)\n",
    "\n",
    "model = Model(input=inputs, output=final_output)\n",
    "# Compile model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, clipnorm=1.)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5006/5006 [==============================] - 7s - loss: 1497.9776 - mean_squared_error: 1497.9771     \n",
      "Epoch 2/3\n",
      "5006/5006 [==============================] - 5s - loss: 0.2126 - mean_squared_error: 0.2126     \n",
      "Epoch 3/3\n",
      "5006/5006 [==============================] - 5s - loss: 0.2104 - mean_squared_error: 0.2104     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15f42f750>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_list, Y_train, nb_epoch=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new_train_list = [val[:10] for val in X_train_list]\n",
    "#test_preds = model.predict(X_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0623922\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "# X_test = np.array(X_test)\n",
    "# X_test_list = [X_test[:,val,:] for val in range(X_test.shape[1])]\n",
    "\n",
    "pred_ranks = model.predict(X_test_list, batch_size=491)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[0][0] = id, [0][1] = topic, [1] = rank\n",
    "ranks_and_keys = zip(test_keys, pred_ranks)\n",
    "result_dict = defaultdict(lambda: [])\n",
    "for val in ranks_and_keys:\n",
    "    result_dict[val[0][1]].append((val[1], val[0][0]))\n",
    "\n",
    "with open('per_term_query_results','wb') as f:\n",
    "    for cur_topic in range(30):\n",
    "        topic_results = result_dict[str(cur_topic)]\n",
    "        topic_results.sort(reverse=True)\n",
    "\n",
    "        for result_rank, cur_result in enumerate(topic_results[:1000]):\n",
    "            line_to_write = [str(cur_topic + 1), '0', str(cur_result[1]), str(result_rank + 1), str(cur_result[0]), 'test_run', '\\n']\n",
    "            f.write(\" \".join(line_to_write))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 37807)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('3071478', '0'), 0.0)\n"
     ]
    }
   ],
   "source": [
    "print ranks_and_keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_list_2 = np.transpose(X_train_list[0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 19.0, 27.0, 35.0, 9.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print test_list_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2794284', '20'), ('3510867', '23'), ('2275225', '21'), ('3658214', '26'), ('2830982', '4'), ('2533397', '5'), ('3838404', '12'), ('1570135', '7'), ('3565924', '6'), ('3274658', '12')]\n"
     ]
    }
   ],
   "source": [
    "print train_dict.keys()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   1  10  93 301 428 335  90  39\n",
      "   9   5   4   0   0   0   0   0   0   0   1]\n",
      "[  0   0   0   0   0   0   0   0   0   0   4  27 189 504 524 399 188  85\n",
      "  35  25   5   1   0   0   1   0   0   0   2]\n",
      "[  0   0   0   0   0   0   0   0   0   0   2  20  84  96 111  62  25  13\n",
      "   2   0   0   0   0   0   0   0   0   0   0]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  9 57 82 56 34  8  8  4  1  0  0  0  0\n",
      "  0  0  0  2]\n",
      "[  0   0   0   0   0   0   0   0   0   0   1  35 150 406 558 502 233 134\n",
      "  54  13   4   3   0   2   1   1   0   0   2]\n",
      "[  0   0   0   0   0   0   0   0   0   0   3  27 110 245 226 179  82  58\n",
      "   8  10   3   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   3  34 117 232 273 135  86  42\n",
      "   6  12   1   0   0   1   1   1   0   0   1]\n",
      "[  0   0   0   0   0   0   0   0   0   0  16  75 258 451 560 394 219  84\n",
      "  54   2   2   0   0   0   0   0   0   0   0]\n",
      "[  0   0   0   0   0   0   0   0   0   0   2   8  66 371 435 424 195  75\n",
      "  60  20   5  10   1   0   0   0   0   0   0]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  2  8 22 63 85 74 47 30 16  8  4  0  0  0  0\n",
      "  0  0  0  1]\n"
     ]
    }
   ],
   "source": [
    "for val in train_dict.keys()[:10]:\n",
    "    print train_dict[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31403, 3, 29)\n"
     ]
    }
   ],
   "source": [
    "thing = [[val, val, val] for val in X_train]\n",
    "print np.array(thing).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print np.array(X_train).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_1 = [shared_dense(val) for val in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'Relu_2:0' shape=(?, 5) dtype=float32>, <tf.Tensor 'Relu_3:0' shape=(?, 5) dtype=float32>, <tf.Tensor 'Relu_4:0' shape=(?, 5) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() got an unexpected keyword argument 'binary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-716d5865b7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Dan/class/deep_ir/project/data/word_vectors.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Dan/anaconda/envs/hw2/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load() got an unexpected keyword argument 'binary'"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load('/Users/Dan/class/deep_ir/project/data/word_vectors.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
